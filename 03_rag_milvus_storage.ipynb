{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Milvus Storage\n",
    "\n",
    "Building on the 2 previous examples we will now add a storage layer to our RAG model. We will use Milvus to store the embeddings of the documents and the queries. We will also use Milvus to perform the similarity search.\n",
    "\n",
    "If you need to setup Milvus there's an included `docker-compose.yml` file that will start a Milvus instance with the required configuration. You can start it with `docker-compose up`.  It's based on the official Milvus `docker-compose.yml` file with an updated Milvus version to match the client in the `requirements.txt` file.\n",
    "\n",
    "## Usage\n",
    "\n",
    "Set the variables below.\n",
    "\n",
    "* `os.environ['ANTHROPIC_API_KEY']` - Your API key from the [Anthropic API](https://www.anthropic.com/).\n",
    "* `ANTRHOPIC_MODEL_ID` - The model ID from the [Anthropic API](https://www.anthropic.com/).\n",
    "* `EMBEDDING_MODEL` - The model to use for the embeddings.\n",
    "* `MILVUS_COLLECTION_ID` - The Milvus collection to store the embeddings.\n",
    "* `MILVUS_HOST` - The Milvus host.\n",
    "* `MILVUS_PORT` - The Milvus port.\n",
    "* `DIMENSIONS` - The number of dimensions of the embeddings.\n",
    "* `DOCUMENT_LIST` - Path to the documents to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ANTHROPIC_MODEL_ID = \"claude-3-5-sonnet-latest\"\n",
    "#os.environ['ANTHROPIC_API_KEY'] = ''\n",
    "EMBEDDING_MODEL = 'BAAI/bge-large-en-v1.5'\n",
    "MILVUS_COLLECTION_ID = 'rag_collection'\n",
    "MILVUS_HOST = 'localhost'\n",
    "MILVUS_PORT = 19530\n",
    "DIMENSIONS = 1024\n",
    "\n",
    "DOCUMENT_LIST = [\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import openai\n",
    "from typing import Optional\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "import markdown\n",
    "import tempfile\n",
    "from sphinx.application import Sphinx\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    Collection,\n",
    "    CollectionSchema,\n",
    "    FieldSchema,\n",
    "    DataType,\n",
    "    utility\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LocalEmbeddingGenerator` class is significantly different from the previous examples.  We've updated the class to use `SentenceTransformers` to generate the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalEmbeddingGenerator:\n",
    "    \"\"\"\n",
    "    A class to generate embeddings for input text using a specified model.\n",
    "    Attributes:\n",
    "    -----------\n",
    "    model : SentenceTransformer\n",
    "        The model used for generating embeddings.\n",
    "    _dimensions : int\n",
    "        The dimensions of the embeddings generated by the model.\n",
    "    Methods:\n",
    "    --------\n",
    "    generate_embedding(text):\n",
    "        Generates embeddings for the input text.\n",
    "    dimensions:\n",
    "        Returns the dimensions of the embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, model=EMBEDDING_MODEL):\n",
    "        # Use BAAI model for 1024 dimensions\n",
    "        self.model = SentenceTransformer(model)\n",
    "        self._dimensions = DIMENSIONS\n",
    "    \n",
    "    def generate_embedding(self, text):\n",
    "        \"\"\"Generate embeddings for input text\"\"\"\n",
    "        embeddings = self.model.encode(text, normalize_embeddings=True)\n",
    "        return embeddings.tolist()\n",
    "    \n",
    "    @property\n",
    "    def dimensions(self):\n",
    "        \"\"\"Return embedding dimensions for collection validation\"\"\"\n",
    "        return self._dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RAGPromptGenerator`, `LLMInterface`, and `ClaudeInterface` classes are the same as the previous examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPromptGenerator:\n",
    "    def __init__(self, rag_system):\n",
    "        self.rag = rag_system\n",
    "    \n",
    "    def generate_prompt(self, query: str, system_prompt: str = None) -> str:\n",
    "        \"\"\"Generate a prompt for the LLM using retrieved context.\"\"\"\n",
    "        context = self.rag.generate_context(query)\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        System: {system_prompt or 'You are a helpful AI assistant. Use the provided context to answer questions accurately. If the context does not contain relevant information, say so.'}\n",
    "        \n",
    "        Context:\n",
    "        {context}\n",
    "        \n",
    "        Human: {query}\n",
    "        \n",
    "        Assistant: Based on the provided context, I'll help answer your question.\n",
    "        \"\"\"\n",
    "        print(prompt)\n",
    "        return prompt.strip()\n",
    "\n",
    "class LLMInterface:\n",
    "    \"\"\"Base class for LLM interactions\"\"\"\n",
    "    def generate_response(self, prompt: str) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "class ClaudeInterface(LLMInterface):\n",
    "    def __init__(self, api_key: Optional[str] = None):\n",
    "        self.api_key = api_key or os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "        self.client = anthropic.Anthropic(api_key=self.api_key)\n",
    "    \n",
    "    def generate_response(self, prompt: str) -> str:\n",
    "        try:\n",
    "            message = self.client.messages.create(\n",
    "                model=ANTHROPIC_MODEL_ID,\n",
    "                max_tokens=1000,\n",
    "                messages=[{\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }]\n",
    "            )\n",
    "            return message.content[0].text\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response from Claude: {str(e)}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MilvusStorage` is a an updated `RAGSystem` class that uses Milvus to store the embeddings.  When the class is initialized, it will create a connection to a Milvus server and create the collection to store embeddings if it doesn't exist.\n",
    "\n",
    "The `create_collection` method is used to create the collection in Milvus.  The schema to our database is configured here.  We store a document id, the embeddings as a vector of floating point numbers, the document text, and the metadata for the document.\n",
    "\n",
    "Similar to before, the `add_document_with_chunking` method is used to generate embeddings and store the documents. The difference is that we are storing the embeddings in Milvus instead of in memory with storage to a local file.\n",
    "\n",
    "`query_similar` allows you to query the Milvus collection for similar documents based on a query string.  The method will generate the embeddings for the query string and then search the Milvus collection for similar embeddings.\n",
    "\n",
    "`generate_context` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilvusRAGSystem:\n",
    "    def __init__(self, collection_name=\"documents\"):\n",
    "        self.collection_name = collection_name\n",
    "        \n",
    "        # Connect to Milvus\n",
    "        connections.connect(alias=\"default\", host=MILVUS_HOST, port=MILVUS_PORT)\n",
    "\n",
    "        self.embedding_generator = LocalEmbeddingGenerator()\n",
    "        \n",
    "        # Initialize collection\n",
    "        if not utility.has_collection(collection_name):\n",
    "            self.create_collection(collection_name)\n",
    "        self.collection = Collection(name=collection_name)\n",
    "        \n",
    "        # Ensure collection is loaded before operations\n",
    "        self.ensure_collection_loaded()\n",
    "\n",
    "    def create_collection(self, collection_name):\n",
    "        # Define schema\n",
    "        fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),  # Primary key field\n",
    "            FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=DIMENSIONS),\n",
    "            FieldSchema(name=\"document\", dtype=DataType.VARCHAR, max_length=65355),\n",
    "            FieldSchema(name=\"metadata\", dtype=DataType.JSON)\n",
    "        ]\n",
    "        schema = CollectionSchema(fields, description=\"Document collection\")\n",
    "        \n",
    "        # Create collection\n",
    "        collection = Collection(name=collection_name, schema=schema)\n",
    "        collection.create_index(field_name=\"embeddings\", index_params={\"index_type\": \"IVF_FLAT\", \"metric_type\": \"COSINE\", \"params\": {\"nlist\": 128}})\n",
    "        collection.load()\n",
    "\n",
    "    def ensure_collection_loaded(self):\n",
    "        \"\"\"Ensure collection is loaded before operations\"\"\"\n",
    "        if not utility.has_collection(self.collection_name):\n",
    "            self.create_collection(self.collection_name)\n",
    "        if not utility.load_state(self.collection_name):\n",
    "            self.collection.load()\n",
    "\n",
    "    def add_document_with_chunking(self, text, metadata, chunk_size=512):\n",
    "        # Split the text into chunks\n",
    "        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "        \n",
    "        documents = []\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            # Get embeddings for each chunk\n",
    "            embeddings = self.embedding_generator.generate_embedding(chunk)  # Implement this based on your embedding model\n",
    "            \n",
    "            # Create document entry for each chunk\n",
    "            document = {\n",
    "                \"document\": chunk,\n",
    "                \"embeddings\": embeddings,\n",
    "                \"metadata\": metadata\n",
    "            }\n",
    "            \n",
    "            documents.append(document)\n",
    "        \n",
    "        # Insert all chunked documents into Milvus\n",
    "        self.collection.insert(documents)\n",
    "\n",
    "    def query_similar(self, query_text, top_k=5):\n",
    "        # Get query embeddings\n",
    "        query_embedding = self.embedding_generator.generate_embedding(query_text)\n",
    "        \n",
    "        # Search in Milvus\n",
    "        search_params = {\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}}\n",
    "        results = self.collection.search(\n",
    "            data=[query_embedding],\n",
    "            anns_field=\"embeddings\",\n",
    "            param=search_params,\n",
    "            limit=top_k,\n",
    "            output_fields=[\"document\", \"metadata\"]\n",
    "        )\n",
    "        \n",
    "        return [\n",
    "            {\n",
    "                \"document\": hit.entity.get(\"document\"),\n",
    "                \"metadata\": hit.entity.get(\"metadata\"),\n",
    "                \"score\": hit.score\n",
    "            }\n",
    "            for hit in results[0]\n",
    "        ]\n",
    "    \n",
    "    def generate_context(self, query: str, num_results: int = 3) -> str:\n",
    "        try:\n",
    "            self.ensure_collection_loaded()  # Ensure loaded before search\n",
    "            \n",
    "            # Generate query embedding\n",
    "            query_embedding = self.embedding_generator.generate_embedding(query)\n",
    "            \n",
    "            # Search collection\n",
    "            search_results = self.collection.search(\n",
    "                data=[query_embedding],\n",
    "                anns_field=\"embeddings\",\n",
    "                param={\"metric_type\": \"COSINE\", \"params\": {\"nprobe\": 10}},\n",
    "                limit=num_results,\n",
    "                output_fields=[\"document\", \"metadata\"]\n",
    "            )\n",
    "            \n",
    "            if not search_results or len(search_results[0]) == 0:\n",
    "                return \"No relevant context found.\"\n",
    "            \n",
    "            # Format context from results\n",
    "            context_parts = []\n",
    "            for hit in search_results[0]:\n",
    "                doc = hit.entity.get('document')  # Correct usage of get method\n",
    "                score = hit.score\n",
    "                metadata = hit.entity.get('metadata')  # Correct usage of get method\n",
    "                source = metadata.get('source', 'Unknown source')\n",
    "                \n",
    "                context_part = f\"\"\"\n",
    "Source: {source}\n",
    "Relevance: {score:.3f}\n",
    "Content: {doc}\n",
    "---\"\"\"\n",
    "                context_parts.append(context_part)\n",
    "            \n",
    "            return \"\\n\".join(context_parts)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Search error: {e}\")\n",
    "            return \"Error retrieving context. Please ensure collection is properly initialized.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RAGWithLLM` is the same as the previous examples.\n",
    "\n",
    "`RAGConversation` enables enhanced conversation capabilities.  The class maintains a history of the conversation and includes it into the prompt when generating subsequent responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGWithLLM:\n",
    "    def __init__(self, rag_system, llm_interface: LLMInterface):\n",
    "        self.rag = rag_system\n",
    "        self.llm = llm_interface\n",
    "        self.prompt_generator = RAGPromptGenerator(rag_system)\n",
    "    \n",
    "    def query(self, user_query: str, system_prompt: Optional[str] = None) -> str:\n",
    "        # Generate RAG-enhanced prompt\n",
    "        enhanced_prompt = self.prompt_generator.generate_prompt(\n",
    "            user_query,\n",
    "            system_prompt\n",
    "        )\n",
    "        \n",
    "        # Get LLM response\n",
    "        response = self.llm.generate_response(enhanced_prompt)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Advanced usage with conversation history\n",
    "class RAGConversation:\n",
    "    def __init__(self, rag_with_llm: RAGWithLLM):\n",
    "        self.rag_with_llm = rag_with_llm\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def add_message(self, role: str, content: str):\n",
    "        self.conversation_history.append({\n",
    "            \"role\": role,\n",
    "            \"content\": content\n",
    "        })\n",
    "    \n",
    "    def query(self, user_query: str) -> str:\n",
    "        # Add user query to history\n",
    "        self.add_message(\"user\", user_query)\n",
    "        \n",
    "        # Generate context-aware prompt including conversation history\n",
    "        context = \"\\n\".join([\n",
    "            f\"{msg['role']}: {msg['content']}\"\n",
    "            for msg in self.conversation_history[-5:]  # Last 5 messages\n",
    "        ])\n",
    "        \n",
    "        # Get response\n",
    "        response = self.rag_with_llm.query(\n",
    "            user_query,\n",
    "            system_prompt=f\"\"\"\n",
    "            Consider the following conversation history:\n",
    "            {context}\n",
    "            \n",
    "            Provide a response that maintains context and continuity.\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        # Add response to history\n",
    "        self.add_message(\"assistant\", response)\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing and loading data.  If we're going to store data in a real database we should process some actual data.  `parse_html`, `parse_markdown`, and `parse_restructuredtext` are used to parse the data from the respective file types.  `load_data` is used to load the data into the Milvus collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def parse_markdown(md_content):\n",
    "    html_content = markdown.markdown(md_content)\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def parse_restructuredtext(rst_content):\n",
    "    # Create a temporary directory for Sphinx to work in\n",
    "    with tempfile.TemporaryDirectory() as temp_dir:\n",
    "        source_dir = os.path.join(temp_dir, 'source')\n",
    "        build_dir = os.path.join(temp_dir, 'build')\n",
    "        os.makedirs(source_dir)\n",
    "        \n",
    "        # Write the reStructuredText content to an index.rst file\n",
    "        with open(os.path.join(source_dir, 'index.rst'), 'w') as f:\n",
    "            f.write(rst_content)\n",
    "        \n",
    "        # Configure Sphinx\n",
    "        conf = {\n",
    "            'extensions': [],\n",
    "            'master_doc': 'index',\n",
    "            'html_theme': 'default',\n",
    "            'exclude_patterns': [],\n",
    "        }\n",
    "        \n",
    "        # Build the HTML using Sphinx\n",
    "        app = Sphinx(\n",
    "            srcdir=source_dir,\n",
    "            confdir=None,\n",
    "            outdir=build_dir,\n",
    "            doctreedir=os.path.join(build_dir, 'doctrees'),\n",
    "            buildername='html',\n",
    "            confoverrides=conf,\n",
    "        )\n",
    "        app.build()\n",
    "        \n",
    "        # Read the generated HTML\n",
    "        with open(os.path.join(build_dir, 'index.html'), 'r') as f:\n",
    "            html_content = f.read()\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "def load_documents(document_list: List[str]) -> List[Dict]:\n",
    "    documents = []\n",
    "    for doc in document_list:\n",
    "        if doc.endswith('.html'):\n",
    "            with open(doc, 'r') as f:\n",
    "                html_content = f.read()\n",
    "                text = parse_html(html_content)\n",
    "        elif doc.endswith('.md'):\n",
    "            with open(doc, 'r') as f:\n",
    "                md_content = f.read()\n",
    "                text = parse_markdown(md_content)\n",
    "        elif doc.endswith('.rst'):\n",
    "            with open(doc, 'r') as f:\n",
    "                rst_content = f.read()\n",
    "                text = parse_restructuredtext(rst_content)\n",
    "        else:\n",
    "            with open(doc, 'r') as f:\n",
    "                text = f.read()\n",
    "        topic = os.path.basename(doc)\n",
    "        documents.append({\n",
    "            'document': text,\n",
    "            'metadata': {\n",
    "                'source': doc,\n",
    "                'topic': topic\n",
    "            }\n",
    "        })\n",
    "    return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "def conversation_example():\n",
    "    # Initialize RAG system\n",
    "    #rag_system = EnhancedRAGSystem()\n",
    "    rag_system = MilvusRAGSystem()\n",
    "    \n",
    "    # Sample documents\n",
    "    documents = load_documents(DOCUMENT_LIST)\n",
    "\n",
    "    \n",
    "    # Add documents to RAG system\n",
    "    for doc in documents:\n",
    "        rag_system.add_document_with_chunking(doc['document'], doc['metadata'])\n",
    "    \n",
    "    # Initialize LLM interface (choose either Claude or ChatGPT)\n",
    "    llm_interface = ClaudeInterface()  # or ChatGPTInterface()\n",
    "    \n",
    "    # Initialize RAG with LLM\n",
    "    rag_with_llm = RAGWithLLM(rag_system, llm_interface)\n",
    "    \n",
    "    conversation = RAGConversation(rag_with_llm)\n",
    "    \n",
    "    # Example conversation\n",
    "    queries = [\n",
    "        \"What is interview insights used for?\",\n",
    "        \"Can you elaborate on its primary features?\",\n",
    "        \"How does this relate to machine learning?\",\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nUser: {query}\")\n",
    "        response = conversation.query(query)\n",
    "        print(f\"Assistant: {response}\")\n",
    "\n",
    "\n",
    "conversation_example()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
