{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic RAG\n",
    "\n",
    "This is the basic setup required for retrieval augmented generation (RAG).  \n",
    "\n",
    "The `LocalEmbeddingGenerator` class is used to generate embeddings for the input text. This class also holds the trained `Word2Vec` model of the input text. \n",
    "\n",
    "The `example_generate_embeddings` function is used to demonstrate the basic usage of the `LocalEmbeddingGenerator` class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/craig/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/craig/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/craig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/craig/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/craig/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sample text for training.', 'Another example of training data.', 'More text to train the embedding model.']\n",
      "Generated embedding for: 'This is a test sentence for embedding generation.'\n",
      "Embedding: [0.0017156526446342468,0.001174519187770784,-0.0018371008336544037,-8.415989577770233e-05,0.00021731418382842094,0.0014967076713219285,-0.0004473058506846428,-0.0007308672065846622,0.0004553204926196486,0.00022061758500058204,0.00031063208007253706,-0.0006860109861008823,-0.0015587980160489678,0.0019070269772782922,0.0019758790731430054,0.002160509815439582,-0.002239283174276352,0.0006865691393613815,-0.0009270735899917781,0.002505313605070114,0.0007561895181424916,0.0012086232891306281,0.0006212539155967534,0.001720957807265222,-0.001495648524723947,0.002055836608633399,-0.000627844303380698,-0.0011879910016432405,-0.0005367162520997226,0.002534780651330948,-0.0017855704063549638,-0.000570760399568826,0.0018231769790872931,-1.451807747798739e-05,-0.0016393143450841308,-0.0016649806639179587,0.002328227972611785,0.0016743686283007264,0.001243123202584684,-0.0008494916255585849,-0.0024134425912052393,0.0009861687431111932,0.0018647266551852226,-0.0014668982475996017,-0.002048180438578129,-0.0007741510053165257,-0.0012843484291806817,-0.0006028935313224792,-0.0005063063581474125,-0.0013717502588406205,0.00246018567122519,-0.0024215448647737503,0.0011729029938578606,0.0014073379570618272,-0.00036699543124996126,0.0023455971386283636,0.002574312500655651,-0.0014257924631237984,-0.0015679687494412065,-0.001757024205289781,-0.0020559588447213173,-0.0007937283371575177,-0.0014567779144272208,-0.002173093846067786,0.00020388078701216727,0.0007798584992997348,0.0016705062007531524,-0.0006846223841421306,-0.0011597595876082778,0.0003254090843256563,0.00010194318747380748,0.002113801660016179,4.760424417327158e-05,0.0018832255154848099,-0.0021522175520658493,0.0021962334867566824,-0.0004919034545309842,0.002265925519168377,-0.0019835513085126877,0.00046780891716480255,0.0002751266583800316,1.1980533599853516e-05,-0.0013289721682667732,-0.0024082546588033438,-0.001891723251901567,-0.0020706180948764086,0.000498366542160511,0.00012460071593523026,-0.00047217123210430145,0.0018542098114266992,-0.0006447114865295589,-0.0003508618101477623,-0.0023178551346063614,-0.002584743080660701,0.0023305723443627357,-0.001498421304859221,-0.0016596348723396659,0.0013540122890844941,0.001736977486871183,-0.0017790732672438025,0.00024993749684654176,-0.001564706675708294,0.00042899572872556746,-0.0011169997742399573,-0.0008960409904830158,0.0005691840196959674,0.0022556192707270384,0.0017521120607852936,-0.0025200669188052416,-0.001464089727960527,0.002052169991657138,0.0005180618609301746,-0.001108346856199205,0.00015594065189361572,0.0024794170167297125,-0.0002871658653020859,-0.002454332774505019,0.00041885673999786377,0.0016230089822784066,0.0016360338777303696,0.0010655339574441314,-0.001471416442655027,-9.65346916927956e-05,-1.4405697584152222e-05,0.0011905716964975,-0.002094163792207837,-0.0020881013479083776,6.894549733260646e-05,-0.0022417446598410606,0.0015156656736508012,-0.00010880517220357433,0.0025966607499867678,-0.0013916607713326812,-0.00012659870844800025,0.0020199930295348167,-0.0010593574261292815,-0.001306224032305181,0.0004140809178352356,0.0006902848253957927,-0.0006679582293145359,0.0016790438676252961,-0.001994779333472252,0.0008837397326715291,0.00012759647506754845,0.0022740059066563845,0.0015579983592033386,0.0017748338868841529,0.00203712098300457,-0.002473989501595497,0.0024901621509343386,-0.0020236342679709196,-0.0006889344076626003,-0.0012777106603607535,-0.0012934105470776558,-0.0020894769113510847,-0.002026975154876709,-0.0011857323115691543,-0.0003321254625916481,-0.0013289032503962517,0.001599099487066269,-0.0024782863911241293,-0.0013820584863424301,0.0024575917050242424,0.001820658682845533,0.0019989104475826025,0.0011027967557311058,0.00013205532741267234,-0.0015575895085930824,0.001567389816045761,0.0006862059235572815,0.002005060901865363,0.001665063202381134,0.002068377798423171,0.0022545328829437494,-0.0025770196225494146,-0.0017592981457710266,0.00034832675009965897,0.0016771623631939292,0.001920264563523233,0.0014367137337103486,0.0019952149596065283,-0.0013347832718864083,0.00171469000633806,-0.0010698888218030334,-0.0023581620771437883,0.0023806458339095116,0.00034671989851631224,-0.000718666531611234,-0.0006452715024352074,-0.0010990829905495048,0.0012532128021121025,0.0011458912631496787,-0.0006909798830747604,-0.0019119480857625604,-0.0009286071290262043,-8.766011887928471e-05,0.0015874719247221947,-0.0007388902013190091,-3.148305040667765e-05,0.00022909541439730674,-0.0018478253623470664,0.0005377608467824757,-0.00037302696728147566,0.0007297263364307582,0.001260995282791555,-0.0003520883619785309,-0.0007239955593831837,0.002015272853896022,0.0013139579677954316,0.0017483128467574716,0.0011759487679228187,0.0022570716682821512,0.0019466066733002663,-0.0002817415224853903,0.002278030849993229,0.0011983640724793077,0.0014168316265568137,-0.00036095952964387834,-0.0005315933376550674,-0.0011521732667461038,-0.0022174993064254522,0.0007910749991424382,0.0023133314680308104,0.0023228495847433805,-0.0005058214883320034,0.0015849368646740913,0.0009843030711635947,-0.0011187409982085228,0.0005320115014910698,-0.0014161160215735435,0.002137732459232211,0.0014148199697956443,0.0008292797137983143,0.0010683772852644324,0.0022544662933796644,0.0018937591230496764,-0.00021705031394958496,-0.0018418667605146766,0.0021824147552251816,0.0018837457755580544,0.00045064371079206467,-0.0003509093075990677,-0.0015338780358433723,-0.001180492341518402,0.002252074657008052,-0.0008164343307726085,-0.0016507342224940658,0.0025703327264636755,0.0020043349359184504,0.0023751670960336924,0.000295703619485721,-0.0021679894998669624,0.002194014610722661,-0.0009625600650906563,0.0014953576028347015,0.0011436404893174767,0.002523423172533512,-0.002420181641355157,0.0023980222176760435,-0.002417064504697919,-0.0017988834297284484,-0.002370363101363182,-0.0014445598935708404,0.0019189832964912057,0.0023865827824920416,-0.0008659769664518535,0.0009695443441160023,-0.0009440633584745228,0.0020524663850665092,0.0015278322389349341,5.432714900166502e-08,-0.0009449673816561699,-0.0018813296919688582,0.0012418270343914628,0.00037837991840206087,-0.0006805171142332256,0.002041095634922385,-0.001054587191902101,-0.002382548525929451,-0.0005873621557839215,3.259039294789545e-05,-0.0017289727693423629,-0.001428806222975254,-0.002213483676314354,0.0024036129470914602,0.001933340565301478,-7.688626646995544e-05,0.0019186623394489288,0.0020705179776996374,-0.0002040555700659752,0.0017218986758962274,0.0009811259806156158,0.001322094351053238,0.001888799830339849,-0.0012342160334810615,-0.0005691492115147412,0.00022737588733434677,0.0011031785979866982,0.0008605029433965683,0.001327038393355906,0.0011943973368033767,-0.0021975282579660416,-0.0008291248232126236,-0.001884572790004313,0.0025212038308382034,0.0013038018951192498,4.448990148375742e-05,0.0010710880160331726,-0.0019937839824706316,-0.0016392320394515991,0.0008011441677808762,0.0017017287900671363,0.0010286131873726845,0.0015671933069825172,-0.0005172218079678714,-0.0008711274713277817,5.3950585424900055e-05,-0.0008318647742271423,-0.001436693943105638,-0.0020282708574086428,0.001701964414678514,-0.0002839419466909021,-0.0004924166132695973,-0.0020324934739619493,0.0024316597264260054,0.0002260785549879074,0.0004608429444488138,0.0006488713552244008,-0.0019234357168897986,0.0004267767071723938,0.0007751465891487896,-0.002230997197329998,0.0012905733892694116,0.0006337001104839146,0.00195258145686239,0.001313619315624237,-0.0007895094458945096,-0.0018653481965884566,0.0018479721620678902,0.0004951913724653423,0.0013539675856009126,0.0016617471119388938,0.0004979893565177917,-0.0015957321738824248,-1.63974857514404e-06,0.002153202658519149,-0.0015881635481491685,0.0024578857701271772,-0.0018726413836702704,0.0011023152619600296,0.0005633840337395668,0.0019376861164346337,-0.0012732461327686906,-0.001188631751574576,-0.0015880661085247993,0.000859210267663002,-0.0011717351153492928,0.002219501184299588,-0.0011168820783495903,-0.0023712031543254852,-0.0012542592594400048,0.0016709609190002084,-0.0016591990133747458,-0.0013701919233426452,-0.0019021903863176703,0.001568297273479402,0.0008743734215386212,0.0007417683373205364,-0.0008173308451659977,0.0015705445548519492,-0.0016022775089368224,-0.0005156512488611042,-0.001558094285428524,-0.00025929170078597963,-0.0005262984777800739,0.002209881553426385,2.0312765627750196e-05,-0.002233157865703106,-0.0014138277620077133,-0.001790621317923069,0.0007011409034021199,0.002462668577209115,-0.0015145823126658797,0.0021523504983633757]\n",
      "[('example', 0.14150157570838928), ('data', 0.08747795224189758), ('sample', 0.0619337260723114), ('text', 0.000784081406891346), ('embedding', -0.007486766204237938), ('model', -0.014921215362846851), ('another', -0.01981394737958908), ('train', -0.024053631350398064)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "\n",
    "class LocalEmbeddingGenerator:\n",
    "    def __init__(self, embedding_dim=384):  # Using 384 dimensions as it's common for many embedding models\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.word2vec_model = None\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Download required NLTK data\n",
    "        import nltk\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('wordnet')\n",
    "        \n",
    "        # Initialize stop words\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Preprocess the input text.\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stop words and lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens \n",
    "                 if token not in self.stop_words and token not in string.punctuation]\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def train_word2vec(self, texts):\n",
    "        \"\"\"Train a Word2Vec model on the given texts.\"\"\"\n",
    "        # Preprocess all texts\n",
    "        print(texts)\n",
    "        processed_texts = [self.preprocess_text(text) for text in texts]\n",
    "        \n",
    "        # Train Word2Vec model\n",
    "        self.word2vec_model = Word2Vec(sentences=processed_texts, \n",
    "                                     vector_size=self.embedding_dim,\n",
    "                                     window=5,\n",
    "                                     min_count=1,\n",
    "                                     workers=4)\n",
    "\n",
    "    def generate_embedding(self, text):\n",
    "        \"\"\"Generate embedding for the input text.\"\"\"\n",
    "        if self.word2vec_model is None:\n",
    "            raise ValueError(\"Word2Vec model not trained. Please train the model first.\")\n",
    "        \n",
    "        # Preprocess the input text\n",
    "        tokens = self.preprocess_text(text)\n",
    "        \n",
    "        if not tokens:\n",
    "            return np.zeros(self.embedding_dim)\n",
    "        \n",
    "        # Get embeddings for each token\n",
    "        token_embeddings = []\n",
    "        for token in tokens:\n",
    "            try:\n",
    "                token_embedding = self.word2vec_model.wv[token]\n",
    "                token_embeddings.append(token_embedding)\n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "        if not token_embeddings:\n",
    "            return np.zeros(self.embedding_dim)\n",
    "        \n",
    "        # Average the token embeddings\n",
    "        final_embedding = np.mean(token_embeddings, axis=0)\n",
    "        \n",
    "        return final_embedding\n",
    "\n",
    "    def format_embedding(self, embedding):\n",
    "        \"\"\"Format the embedding vector as a comma-separated list in square brackets.\"\"\"\n",
    "        return f\"[{','.join(map(str, embedding.tolist()))}]\"\n",
    "\n",
    "# Example usage:\n",
    "def example_generate_embeddings():\n",
    "    import nltk\n",
    "    nltk.download('omw-1.4')\n",
    "    nltk.download('punkt_tab')\n",
    "    # Sample texts for training\n",
    "    training_texts = [\n",
    "        \"This is a sample text for training.\",\n",
    "        \"Another example of training data.\",\n",
    "        \"More text to train the embedding model.\"\n",
    "    ]\n",
    "    \n",
    "    # Initialize the embedding generator\n",
    "    generator = LocalEmbeddingGenerator(embedding_dim=384)\n",
    "    \n",
    "    # Train the model\n",
    "    generator.train_word2vec(training_texts)\n",
    "    \n",
    "    # Generate embedding for new text\n",
    "    test_text = \"This is a test sentence for embedding generation.\"\n",
    "    embedding = generator.generate_embedding(test_text)\n",
    "    \n",
    "    # Format and print the embedding\n",
    "    formatted_embedding = generator.format_embedding(embedding)\n",
    "    print(f\"Generated embedding for: '{test_text}'\")\n",
    "    print(f\"Embedding: {formatted_embedding}\")\n",
    "    print(generator.word2vec_model.wv.most_similar(\"training\"))\n",
    "\n",
    "example_generate_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RAGSystem` class is used to generate the RAG model. This class uses the `LocalEmbeddingGenerator` class to generate embeddings for the input text. The `RAGSystem` class implements the following:\n",
    "\n",
    "* `add_documents` - Add documents to the RAG model and generate embeddings for the documents.\n",
    "* `find_similar_documents` - Find similar documents to the query.\n",
    "* `save_to_disk` - Save the RAG model to disk.\n",
    "* `load_from_disk` - Load the RAG model from disk.\n",
    "\n",
    "We'll build on the `RAGSystem` class in the next notebook to implement the full RAG model.\n",
    "\n",
    "The `example_rag_system` function is used to demonstrate the basic usage of the `RAGSystem` class.\n",
    "\n",
    "The `example_rag_system` function demonstrates the following:\n",
    "\n",
    "1. Create a `RAGSystem` object.\n",
    "2. Create documents and associated metadata.\n",
    "3. Add the documents to the `RAGSystem` object.\n",
    "4. Query the `RAGSystem` object to find similar documents.\n",
    "5. Output the similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python is a high-level programming language known for its simplicity.', 'Machine learning is a subset of artificial intelligence.', 'Natural language processing deals with interaction between computers and human language.', 'Deep learning is a type of machine learning based on artificial neural networks.']\n",
      "\n",
      "Query: What is Python programming?\n",
      "Retrieved documents:\n",
      "- Document: Python is a high-level programming language known for its simplicity.\n",
      "  Metadata: {'source': 'programming_guide', 'category': 'programming'}\n",
      "  Similarity score: 0.5463\n",
      "- Document: Machine learning is a subset of artificial intelligence.\n",
      "  Metadata: {'source': 'ai_textbook', 'category': 'ai'}\n",
      "  Similarity score: -0.0371\n",
      "\n",
      "Query: Tell me about artificial intelligence\n",
      "Retrieved documents:\n",
      "- Document: Machine learning is a subset of artificial intelligence.\n",
      "  Metadata: {'source': 'ai_textbook', 'category': 'ai'}\n",
      "  Similarity score: 0.6557\n",
      "- Document: Deep learning is a type of machine learning based on artificial neural networks.\n",
      "  Metadata: {'source': 'dl_tutorial', 'category': 'deep_learning'}\n",
      "  Similarity score: 0.2791\n",
      "\n",
      "Query: How does NLP work?\n",
      "Retrieved documents:\n",
      "- Document: Deep learning is a type of machine learning based on artificial neural networks.\n",
      "  Metadata: {'source': 'dl_tutorial', 'category': 'deep_learning'}\n",
      "  Similarity score: 0.0000\n",
      "- Document: Natural language processing deals with interaction between computers and human language.\n",
      "  Metadata: {'source': 'nlp_paper', 'category': 'nlp'}\n",
      "  Similarity score: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/craig/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/craig/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/craig/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self, embedding_dim=384):\n",
    "        self.embedding_generator = LocalEmbeddingGenerator(embedding_dim)\n",
    "        self.document_store: List[Dict] = []\n",
    "        self.document_embeddings: List[np.ndarray] = []\n",
    "\n",
    "    def add_documents(self, documents: List[str], metadata: List[Dict] = None):\n",
    "        \"\"\"\n",
    "        Add documents to the RAG system and generate their embeddings.\n",
    "        \n",
    "        Args:\n",
    "            documents: List of document texts\n",
    "            metadata: Optional list of metadata dictionaries for each document\n",
    "        \"\"\"\n",
    "        # Train the embedding model on all documents\n",
    "        self.embedding_generator.train_word2vec(documents)\n",
    "\n",
    "        # Generate embeddings and store documents\n",
    "        for i, doc in enumerate(documents):\n",
    "            embedding = self.embedding_generator.generate_embedding(doc)\n",
    "            \n",
    "            doc_entry = {\n",
    "                'id': len(self.document_store),\n",
    "                'text': doc,\n",
    "                'metadata': metadata[i] if metadata else {}\n",
    "            }\n",
    "            \n",
    "            self.document_store.append(doc_entry)\n",
    "            self.document_embeddings.append(embedding)\n",
    "\n",
    "    def find_similar_documents(self, query: str, k: int = 3) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"\n",
    "        Find the k most similar documents to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            k: Number of documents to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List of tuples containing (document, similarity_score)\n",
    "        \"\"\"\n",
    "        # Generate embedding for the query\n",
    "        query_embedding = self.embedding_generator.generate_embedding(query)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(\n",
    "            [query_embedding],\n",
    "            self.document_embeddings\n",
    "        )[0]\n",
    "        \n",
    "        # Get top k similar documents\n",
    "        top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            results.append((self.document_store[idx], similarities[idx]))\n",
    "            \n",
    "        return results\n",
    "\n",
    "    def save_to_disk(self, filepath: str):\n",
    "        \"\"\"Save the RAG system to disk.\"\"\"\n",
    "        data = {\n",
    "            'documents': self.document_store,\n",
    "            'embeddings': [emb.tolist() for emb in self.document_embeddings]\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f)\n",
    "\n",
    "    def load_from_disk(self, filepath: str):\n",
    "        \"\"\"Load the RAG system from disk.\"\"\"\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "        self.document_store = data['documents']\n",
    "        self.document_embeddings = [np.array(emb) for emb in data['embeddings']]\n",
    "\n",
    "    \n",
    "# Example query\n",
    "def example_basic_rag_model():\n",
    "    # Initialize RAG system\n",
    "    rag = RAGSystem()\n",
    "    \n",
    "    # Sample documents\n",
    "    documents = [\n",
    "        \"Python is a high-level programming language known for its simplicity.\",\n",
    "        \"Machine learning is a subset of artificial intelligence.\",\n",
    "        \"Natural language processing deals with interaction between computers and human language.\",\n",
    "        \"Deep learning is a type of machine learning based on artificial neural networks.\",\n",
    "    ]\n",
    "    \n",
    "    # Add metadata for each document\n",
    "    metadata = [\n",
    "        {'source': 'programming_guide', 'category': 'programming'},\n",
    "        {'source': 'ai_textbook', 'category': 'ai'},\n",
    "        {'source': 'nlp_paper', 'category': 'nlp'},\n",
    "        {'source': 'dl_tutorial', 'category': 'deep_learning'}\n",
    "    ]\n",
    "    \n",
    "    # Add documents to RAG system\n",
    "    rag.add_documents(documents, metadata)\n",
    "    \n",
    "    # Example queries and retrieval\n",
    "    queries = [\n",
    "        \"What is Python programming?\",\n",
    "        \"Tell me about artificial intelligence\",\n",
    "        \"How does NLP work?\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        similar_docs = rag.find_similar_documents(query, k=2)\n",
    "        \n",
    "        print(\"Retrieved documents:\")\n",
    "        for doc, similarity in similar_docs:\n",
    "            print(f\"- Document: {doc['text']}\")\n",
    "            print(f\"  Metadata: {doc['metadata']}\")\n",
    "            print(f\"  Similarity score: {similarity:.4f}\")\n",
    "\n",
    "example_basic_rag_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
